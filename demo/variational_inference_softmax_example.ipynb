{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Variational Inference for a Softmax Model\n",
    "\n",
    "The local variational method can be extended to multidimensional models by use of the softmax function (see e.g. Ahmed, 2013). In this demo we consider the following model:\n",
    "\n",
    "\\begin{align*}\n",
    "    x_t &\\sim \\mathcal{N}(A x_{t-1}, 0.01 I)\\\\\n",
    "    y_t &\\sim \\mathcal{C}at(\\sigma(x_t))\\,,\n",
    "\\end{align*}\n",
    "\n",
    "with $\\sigma$ a softmax. In this demo, we extend upon this by using the softmax to implement a \"greater than\" constraint as used in the example from Infer.NET, 2020, see references. The example consists of a number of match results in head-to-head encounters between 5 players being used to estimate their (relative) skills, including an estimate of the uncertainty on each skill. This notebook was developed by Keith Myerscough of Sioux LIME, lending heavily from other notebooks in this demo folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using ForneyLab\n",
    "using PyPlot\n",
    "using LinearAlgebra\n",
    "\n",
    "# Generate data set\n",
    "Random.seed!(21)\n",
    "σ(x) = exp.(x)/sum(exp.(x)) # Softmax function\n",
    "\n",
    "n_players = 5\n",
    "winners = [0, 0, 0, 1, 3, 4] .+ 1  # .+ 1 for Julia-indexing\n",
    "losers = [1, 3, 4, 2, 1, 2] .+ 1\n",
    "n_matches = length(winners)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model specification\n",
    "\n",
    "The model specification includes local variational parameters `xi` and `a`, which are used to define an upperbound on the softmax (Bouchard, 2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = FactorGraph()\n",
    "\n",
    "perf_var = 1\n",
    "perf_prec = 1/perf_var^2\n",
    "\n",
    "m_s_prior = 6. * ones(n_players)\n",
    "v_s_prior = 9. * eye(n_players)\n",
    "\n",
    "width = 2\n",
    "\n",
    "@RV s ~ GaussianMeanVariance(m_s_prior, v_s_prior)\n",
    "p = Vector{Variable}(undef, n_matches)\n",
    "xi = Vector{Variable}(undef, n_matches)\n",
    "a = Vector{Variable}(undef, n_matches)\n",
    "y = Vector{Variable}(undef, n_matches)\n",
    "\n",
    "for i_m = 1:n_matches\n",
    "    println(\"$(winners[i_m]) beats $(losers[i_m])\")\n",
    "    A = zeros(2, n_players)\n",
    "    A[1, winners[i_m]] = 1\n",
    "    A[2, losers[i_m]] = 1\n",
    "#     println(A)\n",
    "    @RV p[i_m] ~ GaussianMeanPrecision(A * s, perf_prec*eye(2))\n",
    "    @RV xi[i_m]\n",
    "    @RV a[i_m]\n",
    "    @RV y[i_m] ~ Softmax(p[i_m], xi[i_m], a[i_m])\n",
    "\n",
    "    # Data placeholder\n",
    "    placeholder(y[i_m], :y, index=i_m, dims=(2,))\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForneyLab.draw(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm generation\n",
    "\n",
    "Since we are interested in optimizing the local variational parameters `xi`, `a` together with the hidden state sequence `x`, we construct an algorithm that also updates `xi` and `a`. We can also generate an algorithm for evaluating the free energy. However, because we upper-bound the softmax, the free energy is no longer guaranteed to be a upper bound on surprise. This is in contrast to local variational estimation for the logistic function, which is lower bounded (see the corresponding demo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = variationalAlgorithm(s, p, xi, a, ids=[:S, :P, :Xi, :A], free_energy=true)\n",
    "source_code = algorithmSourceCode(algo, free_energy=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# println(source_code) # Uncomment to inspect algorithm code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution\n",
    "\n",
    "For execution we initialize the local variational parameters and iterate the automatically derived algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(Meta.parse(source_code));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-initialize marginals\n",
    "marginals = Dict()\n",
    "\n",
    "marginals[:s] = ProbabilityDistribution(Multivariate, GaussianMeanVariance, \n",
    "    m=m_s_prior, v=v_s_prior)\n",
    "for t=1:n_matches\n",
    "    marginals[:p_*t] = ProbabilityDistribution(Multivariate, GaussianMeanVariance, \n",
    "        m=m_s_prior[1] * ones(2), v=v_s_prior[1, 1] * eye(2))\n",
    "    marginals[:xi_*t] = ProbabilityDistribution(Multivariate, GaussianMeanPrecision, m=1. * ones(2), w=1. * eye(2))\n",
    "    marginals[:a_*t] = ProbabilityDistribution(Univariate, GaussianMeanPrecision)\n",
    "end\n",
    "data = Dict(:y  => [[1, 0] for i_m=1:n_matches])\n",
    "\n",
    "n_its = 100\n",
    "F = Vector{Float64}(undef, n_its)\n",
    "\n",
    "println(\"$(mean(marginals[:s])[1]) ± $(var(marginals[:s])[1])\")\n",
    "for i = 1:n_its\n",
    "    stepA!(data, marginals)\n",
    "    stepXi!(data, marginals) # Update local variational parameters\n",
    "    stepS!(data, marginals) # Update hidden state\n",
    "    stepP!(data, marginals) # Update hidden state\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Results show that the algorithm accurately estimates the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior state statistics\n",
    "m_s = [mean(marginals[:s])[t] for t = 1:n_players]\n",
    "v_s = [cov(marginals[:s])[t, t] for t = 1:n_players]\n",
    "m_p_1 = [mean(marginals[:p_*t])[1] for t = 1:n_matches]\n",
    "v_p_1 = [cov(marginals[:p_*t])[1, 1] for t = 1:n_matches]\n",
    "m_p_2 = [mean(marginals[:p_*t])[2] for t = 1:n_matches]\n",
    "v_p_2 = [cov(marginals[:p_*t])[2, 2] for t = 1:n_matches]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = sortperm(m_s - 3*v_s)\n",
    "for i_p=n_players:-1:1\n",
    "    j_p = order[i_p]\n",
    "    println(\"player $(j_p-1) with rating $(m_s[j_p]) ± $(3*sqrt.(v_s[j_p]))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is compared to that of Infer.NET, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_mean = [9.517, 4.955, 2.639, 6.834, 6.054]\n",
    "ref_dev = [3.926, 3.503, 4.288, 3.892, 4.731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ref_mean, m_s)\n",
    "plt.plot([2, 10], [2, 10])\n",
    "m_s ./ ref_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ref_dev, v_s)\n",
    "v_s, ref_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the esimated skills are very close to the reference of Infer.NET, the variances are not. This might be due to the current results being based on Variance Message Passing, while Infer.NET uses Expectation Propagation. The key difference is the order of arguments in the underlying Kullback-Leibler divergence. Using Expectation Propagation in ForneyLab has not proven successful yet; this is work-in-progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Bouchard, 2007 \"Efficient Bounds for the Softmax Function\"\n",
    "\n",
    "Ahmed, 2013, \"Bayesian Multicategorical Soft Data Fusion for Human-Robot Collaboration\"\n",
    "\n",
    "Infer.NET, 2020, \"https://docs.microsoft.com/en-us/dotnet/machine-learning/how-to-guides/matchup-app-infer-net\""
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
